from person import Person
import pandas as pd
import numpy as np
import time
from datetime import datetime
from time import mktime
import seaborn as sns
import matplotlib.pyplot as plt
# used to remove common, useless words
from nltk.corpus import stopwords
# used to perform an analysis on words, sentiment analysis etc.
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report


#pipeline = Pipeline([
#    ('bow', CountVectorizer()),  # strings to token integer counts
#    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
#    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier
#])

# import can be put in a try-except to work if some packages are not installed
# import install_requirements

def classify_msg(chat_data):
    X = chat_data[chat_data['Content'] != '<Media omitted>\n']['Content'].values
    y = chat_data[chat_data['Content'] != '<Media omitted>\n']['Content'].index.get_level_values(level='Person')

    pipeline = Pipeline([
        ('bow', CountVectorizer()),  # strings to token integer counts
        # ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
        ('classifier', MultinomialNB())  # train on TF-IDF vectors w/ Naive Bayes classifier
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    pipeline.fit(X_train, y_train)
    predictions = pipeline.predict(X_test)

    print(classification_report(predictions, y_test))

def chat_fix(messages, timestamps):
    # Fixing the messages list from errors resulting from the multiline messages in the original file and creating a dates list
    for i in range(len(messages)):
        try:
            timestamps.append(time.strptime(messages[i].split('-')[0].split(',')[0] + messages[i].split('-')[0].split(',')[1], '%m/%d/%y %H:%M '))
        except:
            num = 1
            while True:
                if messages[i - num] != 'NaN':
                    messages[i - num] += (' ' + messages[i])
                    messages[i] = 'NaN'
                    break
                else:
                    num += 1


def remove_nan(messages):
    while True:
        try:
            messages.remove('NaN')
        except ValueError:
            break

def count_words(x):
    if x == '<Media omitted>\n':
        return 0
    else:
        return len(x.split())

# Reading from the file generated by WhatsApp
with open('WhatsApp Chat with Sara.txt', 'r') as chat:
    messages = chat.readlines()

people = []
names = []
person = []
timestamps = []
content = []
times = []
hours = []

day = []
month = []
year = []

words_by_month = pd.DataFrame()

chat_fix(messages, timestamps)

remove_nan(messages)

for message in messages:
    try:
        content.append(message.split('-')[1].split(':', maxsplit=1)[1].replace(' ', '', 1))
        person.append(message.split('-')[1].split(':', maxsplit=1)[0].replace(' ', '', 1))
    except:
        content.append(message.split('-')[1].replace(' ', '', 1))
        person.append('-')

for timestamp in timestamps:
    day.append(datetime.fromtimestamp(mktime(timestamp)).date().day)
    month.append(datetime.fromtimestamp(mktime(timestamp)).date().month)
    year.append(datetime.fromtimestamp(mktime(timestamp)).date().year)
    times.append(datetime.fromtimestamp(mktime(timestamp)).time())
    hours.append(datetime.fromtimestamp(mktime(timestamp)).hour)

index = pd.MultiIndex.from_arrays([year, month, day, hours, person], names=['Year', 'Month', 'Day', 'Hour', 'Person'])

chat_data = pd.DataFrame({'Time': times, 'Content': content}, index=index)
# chat_data = pd.DataFrame(list(zip(dates, hours, content, person)), columns = ['Date', 'Time', 'Content', 'Person'])
chat_data.drop('-', level='Person', inplace=True)

chat_data['Word count'] = chat_data['Content'].apply(count_words)

# list of names of people in the conversation
names = set(person)
names.remove('-')

# TO DO
# general.calendar_plot(df, year=2017, how='count', column='index')
# add to the chat_data dataframe info about number of words and letter for each message and length
# pandas can do this df['date'] = pd.to_datetime(df['date']), in Keras 2 notebook maybe a better way to extract year and month
# .rstrip() for removing whitespaces from strings, maybe useful
# clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]
# I might need to first download stopwords # nltk.download_shell()
# nltk stemming can be interesting #not really after all
# train_data.itertuples() allows to iterate over the rows of a dataframe
# average time between responding to texts
# check if words per hour count is correct
# Properly make a new dataframe with features calculated and indexed per month and per person
# maybe do the same per hour as well, then need to add an hour to the multi index
# maybe useful embark = pd.get_dummies(train['Embarked'], drop_first=True)
# saving text to file
# Explore plt.figure and how it can link to the seaborn plot
# Groupby can be done based on multiple columns df.groupby(['Day of Week', 'Hour']).count()
# improve printing of the most common words
# Graphs for monthly number of messages
# Monthly word averages
# Monthly word count
# Most common time of chatting, most common day, month as well

# This loop creates a person object for each person participating in the chat
for name in names:
    people.append(Person(name))

for person in people:
    person.count_messages(chat_data)
    person.most_common_words(chat_data)
    person.count_total_words(chat_data)
    person.calculate_average()
    person.count_media(chat_data)
    #person.count_words_by_month_day_hour(chat_data)

    #words_by_month[person.name] = person.word_count_by_month
    print(person)
'''
words_by_month = pd.DataFrame(words_by_month.stack())
classify_msg(chat_data)

sns.set_style('darkgrid')

# plot of the amount of messages per month per person (here, specifically for 2019)
figure1, ax1 = plt.subplots()
sns.countplot(x=chat_data.loc[2019].index.get_level_values(level='Month'), data=chat_data.loc[2019], hue=chat_data.loc[2019].index.get_level_values(level='Person'))
plt.ylabel('Number of Messages')
figure1.savefig('MSGbyMonthPlot.png')

# plot of the amount of messages per day per person (here, specifically for 2019)
figure2, ax2 = plt.subplots()
sns.countplot(x=chat_data.loc[2019].index.get_level_values(level='Day'), data=chat_data.loc[2019], hue=chat_data.loc[2019].index.get_level_values(level='Person'))
plt.ylabel('Number of Messages')
figure2.savefig('MSGbyDayPlot.png')

# plot of the amount of words per month per person (here, specifically for 2019)
figure3, ax3 = plt.subplots()
sns.barplot(x=words_by_month.index.get_level_values(level=0), y=words_by_month[0], hue=words_by_month.index.get_level_values(level=1))
#sns.barplot(x=words_sum_sara.index, y=words_sum_sara, alpha=0.5, color='red')
#sns.barplot(x=words_sum_kamil.index, y=words_sum_kamil, alpha=0.5, color='blue')
plt.ylabel('Number of Words')
figure3.savefig('WordsByMonthPlot.png')

# plot of the amount of messages per hour per person (here, specifically for 2019)
figure4, ax4 = plt.subplots()
sns.countplot(x=chat_data.loc[2019].index.get_level_values(level='Hour'), data=chat_data.loc[2019], hue=chat_data.loc[2019].index.get_level_values(level='Person'))
plt.ylabel('Number of Messages')
figure4.savefig('MSGbyHourPlot.png')
'''
#per hour
words_sara_hour = chat_data.loc[2019][chat_data.loc[2019]['Content'] != '<Media omitted>\n'].xs('Sara', level='Person')['Content'].apply(lambda x: len(x.split()))
words_sum_sara_hour = words_sara_hour.groupby('Hour').sum()
words_kamil_hour = chat_data.loc[2019][chat_data.loc[2019]['Content'] != '<Media omitted>\n'].xs('Kamil Ku≈∫niak', level='Person')['Content'].apply(lambda x: len(x.split()))
words_sum_kamil_hour = words_kamil_hour.groupby('Hour').sum()
words_by_hour = pd.DataFrame({'Sara': words_sum_sara_hour, 'Kamil': words_sum_kamil_hour})
words_by_hour = pd.DataFrame(words_by_hour.stack())

figure5, ax5 = plt.subplots()
sns.barplot(x=words_by_hour.index.get_level_values(level=0), y=words_by_hour[0], hue=words_by_hour.index.get_level_values(level=1))
#sns.barplot(x=words_sum_sara.index, y=words_sum_sara, alpha=0.5, color='red')
#sns.barplot(x=words_sum_kamil.index, y=words_sum_kamil, alpha=0.5, color='blue')
plt.ylabel('Number of Words')
figure5.savefig('WordsByHourPlot.png')
'''
plt.tight_layout()
plt.show()

# print(chat_data)

# print('Kamil sent ' + str(round((1 - kamil_msg/sara_msg)*100)) + '% less messages than Sara')
# print('Kamil sent ' + str(round((1 - kamil_words / sara_words) * 100)) + '% less words than Sara')

# if __name__ == '__main__':
'''