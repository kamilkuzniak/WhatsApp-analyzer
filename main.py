from person import Person
import pandas as pd
import numpy as np
import time
from datetime import datetime
from time import mktime
import seaborn as sns
import matplotlib.pyplot as plt
# used to remove common, useless words
from nltk.corpus import stopwords
# used to perform an analysis on words, sentiment analysis etc.
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix


#pipeline = Pipeline([
#    ('bow', CountVectorizer()),  # strings to token integer counts
#    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
#    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier
#])

# import can be put in a try-except to work if some packages are not installed
# import install_requirements

def classify_msg(chat_data):
    X = chat_data[chat_data['Content'] != '<Media omitted>\n']['Content'].values
    y = chat_data[chat_data['Content'] != '<Media omitted>\n']['Content'].index.get_level_values(level='Person')

    pipeline = Pipeline([
        ('bow', CountVectorizer()),  # strings to token integer counts
        # ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
        ('classifier', MultinomialNB())  # train on TF-IDF vectors w/ Naive Bayes classifier
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    pipeline.fit(X_train, y_train)
    predictions = pipeline.predict(X_test)

    print(classification_report(predictions, y_test))
    print('\n')
    print(confusion_matrix(predictions, y_test))

def chat_fix(messages, timestamps):
    # Fixing the messages list from errors resulting from the multiline messages in the original file and creating a dates list
    for i in range(len(messages)):
        try:
            timestamps.append(time.strptime(messages[i].split('-')[0].split(',')[0] + messages[i].split('-')[0].split(',')[1], '%m/%d/%y %H:%M '))
        except:
            num = 1
            while True:
                if messages[i - num] != 'NaN':
                    messages[i - num] += (' ' + messages[i])
                    messages[i] = 'NaN'
                    break
                else:
                    num += 1


def remove_nan(messages):
    while True:
        try:
            messages.remove('NaN')
        except ValueError:
            break

def count_words(x):
    if x == '<Media omitted>\n':
        return 0
    else:
        return len(x.split())

# Reading from the file generated by WhatsApp
with open('WhatsApp Chat with Sara.txt', 'r') as chat:
    messages = chat.readlines()

people = []
names = []
person = []
timestamps = []
content = []
times = []
hours = []

day = []
month = []
year = []

words_by_month = pd.DataFrame()
words_by_hour = pd.DataFrame()

chat_fix(messages, timestamps)

remove_nan(messages)

for message in messages:
    try:
        content.append(message.split('-')[1].split(':', maxsplit=1)[1].replace(' ', '', 1))
        person.append(message.split('-')[1].split(':', maxsplit=1)[0].replace(' ', '', 1))
    except:
        content.append(message.split('-')[1].replace(' ', '', 1))
        person.append('-')

for timestamp in timestamps:
    day.append(datetime.fromtimestamp(mktime(timestamp)).date().day)
    month.append(datetime.fromtimestamp(mktime(timestamp)).date().month)
    year.append(datetime.fromtimestamp(mktime(timestamp)).date().year)
    times.append(datetime.fromtimestamp(mktime(timestamp)).time())
    hours.append(datetime.fromtimestamp(mktime(timestamp)).hour)

index = pd.MultiIndex.from_arrays([year, month, day, hours, person], names=['Year', 'Month', 'Day', 'Hour', 'Person'])

chat_data = pd.DataFrame({'Time': times, 'Content': content}, index=index)
# chat_data = pd.DataFrame(list(zip(dates, hours, content, person)), columns = ['Date', 'Time', 'Content', 'Person'])
chat_data.drop('-', level='Person', inplace=True)

chat_data['Word count'] = chat_data['Content'].apply(count_words)

# list of names of people in the conversation
names = set(person)
names.remove('-')

# TO DO
# change file opening to more general
# add documentation
# check if words per hour count is correct
# saving text to file
# Explore plt.figure and how it can link to the seaborn plot
# improve printing of the most common words

# This loop creates a person object for each person participating in the chat
for name in names:
    people.append(Person(name))

text_file = open('Output.txt', 'w')

for person in people:
    person.count_messages(chat_data)
    person.most_common_words(chat_data)
    person.count_total_words(chat_data)
    person.calculate_average()
    person.count_media(chat_data)
    person.count_words_by_month_day_hour(chat_data)

    print(person)
    text_file.write(str(person))

    words_by_month[person.name] = person.word_count_by_month
    words_by_hour[person.name] = person.word_count_by_hour

text_file.close()

classify_msg(chat_data)

sns.set_style('darkgrid')
'''
# plot of the amount of messages per month per person (here, specifically for 2019)
figure1, ax1 = plt.subplots()
sns.countplot(x=chat_data.loc[2019].index.get_level_values(level='Month'), data=chat_data.loc[2019], hue=chat_data.loc[2019].index.get_level_values(level='Person'))
plt.ylabel('Number of Messages')
figure1.savefig('MSGbyMonthPlot.png')

# plot of the amount of messages per day per person (here, specifically for 2019)
figure2, ax2 = plt.subplots()
sns.countplot(x=chat_data.loc[2019].index.get_level_values(level='Day'), data=chat_data.loc[2019], hue=chat_data.loc[2019].index.get_level_values(level='Person'))
plt.ylabel('Number of Messages')
figure2.savefig('MSGbyDayPlot.png')

# plot of the amount of words per month per person (here, specifically for 2019)
figure3, ax3 = plt.subplots()
words_by_month.plot.bar(ax=ax3)
plt.ylabel('Number of Words')
figure3.savefig('WordsByMonthPlot.png')

# plot of the amount of messages per hour per person (here, specifically for 2019)
figure4, ax4 = plt.subplots()
sns.countplot(x=chat_data.loc[2019].index.get_level_values(level='Hour'), data=chat_data.loc[2019], hue=chat_data.loc[2019].index.get_level_values(level='Person'))
plt.ylabel('Number of Messages')
figure4.savefig('MSGbyHourPlot.png')

# plot of the amount of words per hour per person (here, specifically for 2019)
figure5, ax5 = plt.subplots()
words_by_hour.plot.bar(ax=ax5)
plt.ylabel('Number of Words')
figure5.savefig('WordsByHourPlot.png')

plt.tight_layout()
plt.show()
'''
#with open('Output.txt', 'w') as text_file:
#    text_file.write()

# print('Kamil sent ' + str(round((1 - kamil_msg/sara_msg)*100)) + '% less messages than Sara')
# print('Kamil sent ' + str(round((1 - kamil_words / sara_words) * 100)) + '% less words than Sara')

# if __name__ == '__main__':
