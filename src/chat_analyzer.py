from importlib import import_module
import sys
import time
from src.person.person import Person
from datetime import datetime
from time import mktime
# used to perform an analysis on words, sentiment analysis etc.
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix


named_libs = [('pandas', 'pd'), ('seaborn', 'sns'), ('matplotlib.pyplot', 'plt')]
for (name, short) in named_libs:
    try:
        lib = import_module(name)
    except:
        print(sys.exc_info())
    else:
        globals()[short] = lib


# import can be put in a try-except to work if some packages are not installed
# import install_requirements - created on my own file with all the requirements

def classify_msg(chat_data):
    """
    Predicts the author of each message using bag of words and naive Bayes
    :param chat_data: Dataframe with the messages from the chat
    :return: string of classification report and confusion matrix
    """
    X = chat_data[chat_data['Content'] != '<Media omitted>\n']['Content'].values
    y = chat_data[chat_data['Content'] != '<Media omitted>\n']['Content'].index.get_level_values(level='Person')

    pipeline = Pipeline([
        ('bow', CountVectorizer()),  # strings to token integer counts
        # ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
        ('classifier', MultinomialNB())  # train on TF-IDF vectors w/ Naive Bayes classifier
    ])

    # splitting the given messages into the training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    pipeline.fit(X_train, y_train)
    predictions = pipeline.predict(X_test)

    print(classification_report(predictions, y_test))
    print('\n')
    print(confusion_matrix(predictions, y_test))

    return str(classification_report(predictions, y_test)) + '\n' + 'Confusion matrix' + '\n' + str(
        confusion_matrix(predictions, y_test))


def chat_fix(messages):
    """
    Fixing the messages list errors resulting from the multiline messages
    in the original file and creating the dates list
    :param messages: list of messages read directly from the WhatsApp file
    :return: timestamps: list of timestamps extracted from the messages
    """
    timestamps = []
    # this loop checks if there is a timestamp at the beginning of the message, if not, it adds the content
    # to the previous message and replaces the current one with NaN
    for i in range(len(messages)):
        try:
            timestamps.append(
                time.strptime(messages[i].split('-')[0].split(',')[0] + messages[i].split('-')[0].split(',')[1],
                              '%m/%d/%y %H:%M '))
        except:
            num = 1
            while True:
                if messages[i - num] != 'NaN':
                    messages[i - num] += (' ' + messages[i])
                    messages[i] = 'NaN'
                    break
                else:
                    num += 1
    return timestamps


def remove_nan(messages):
    """
    Removes the NaN messages created as a result of fixing the chat
    :param messages: list of messages modified by the chat_fix
    """
    while True:
        try:
            messages.remove('NaN')
        except ValueError:
            break


def count_words(msg):
    """
    Counts the number of words in a message, if the message was media, returns 0
    :param msg: single message from the chat
    :return: number of words in the message
    """
    if msg == '<Media omitted>\n':
        return 0
    else:
        return len(msg.split())


# Reading from the file generated by WhatsApp
with open('data/chat.txt', 'r') as chat:
    messages = chat.readlines()

people = []
names = []
person = []
content = []
times = []
hours = []

day = []
month = []
year = []

words_by_month = pd.DataFrame()
words_by_hour = pd.DataFrame()

timestamps = chat_fix(messages)

remove_nan(messages)

# This for loop creates the lists of content and person labels to add later to the Dataframe
for message in messages:
    try:
        content.append(message.split('-')[1].split(':', maxsplit=1)[1].replace(' ', '', 1))
        person.append(message.split('-')[1].split(':', maxsplit=1)[0].replace(' ', '', 1))
    except:
        content.append(message.split('-')[1].replace(' ', '', 1))
        person.append('-')

# This for loop creates day, month, year, hours and full time lists to add later to the Dataframe
for timestamp in timestamps:
    day.append(datetime.fromtimestamp(mktime(timestamp)).date().day)
    month.append(datetime.fromtimestamp(mktime(timestamp)).date().month)
    year.append(datetime.fromtimestamp(mktime(timestamp)).date().year)
    times.append(datetime.fromtimestamp(mktime(timestamp)).time())
    hours.append(datetime.fromtimestamp(mktime(timestamp)).hour)

# Creates the multiline index that simplifies sorting of the Dataframe by different time frames
index = pd.MultiIndex.from_arrays([year, month, day, hours, person], names=['Year', 'Month', 'Day', 'Hour', 'Person'])

chat_data = pd.DataFrame({'Time': times, 'Content': content}, index=index)
chat_data.drop('-', level='Person', inplace=True)

chat_data['Word count'] = chat_data['Content'].apply(count_words)

# list of names of people in the conversation
names = set(person)
names.remove('-')

# TO DO
# change file opening to more general
# make the requirement file
# make a better folder structure for the project
# write some unit tests, pylint, unittest

# This loop creates a person object for each person participating in the chat
for name in names:
    people.append(Person(name))

text_file = open('../Output.txt', 'w')

# This for loop calculates all the metrics for each person participating in the chat
# and creates the output text file and dataframes for the plots
for person in people:
    person.count_messages(chat_data)
    person.most_common_words(chat_data)
    person.count_total_words(chat_data)
    person.calculate_average()
    person.count_media(chat_data)
    person.count_words_by_month_day_hour(chat_data)

    print(person)
    text_file.write(str(person))

    words_by_month[person.name] = person.word_count_by_month
    words_by_hour[person.name] = person.word_count_by_hour

text_file.write(classify_msg(chat_data))

text_file.close()

sns.set_style('darkgrid')

# plot of the amount of messages per month per person (here, specifically for 2019)
figure1, ax1 = plt.subplots()
sns.countplot(x=chat_data.loc[2019].index.get_level_values(level='Month'), data=chat_data.loc[2019], hue=chat_data.loc[2019].index.get_level_values(level='Person'), ax=ax1)
plt.ylabel('Number of Messages')
figure1.savefig('MSGbyMonthPlot.png')

# plot of the amount of messages per day per person (here, specifically for 2019)
figure2, ax2 = plt.subplots()
sns.countplot(x=chat_data.loc[2019].index.get_level_values(level='Day'), data=chat_data.loc[2019], hue=chat_data.loc[2019].index.get_level_values(level='Person'), ax=ax2)
plt.ylabel('Number of Messages')
figure2.savefig('MSGbyDayPlot.png')

# plot of the amount of words per month per person (here, specifically for 2019)
figure3, ax3 = plt.subplots()
words_by_month.plot.bar(ax=ax3)
plt.ylabel('Number of Words')
figure3.savefig('WordsByMonthPlot.png')

# plot of the amount of messages per hour per person (here, specifically for 2019)
figure4, ax4 = plt.subplots()
sns.countplot(x=chat_data.loc[2019].index.get_level_values(level='Hour'), data=chat_data.loc[2019], hue=chat_data.loc[2019].index.get_level_values(level='Person'), ax=ax4)
plt.ylabel('Number of Messages')
figure4.savefig('MSGbyHourPlot.png')

# plot of the amount of words per hour per person (here, specifically for 2019)
figure5, ax5 = plt.subplots()
words_by_hour.plot.bar(ax=ax5)
plt.ylabel('Number of Words')
figure5.savefig('WordsByHourPlot.png')

plt.tight_layout()
plt.show()

# if __name__ == '__main__':
